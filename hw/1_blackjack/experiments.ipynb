{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6256994",
   "metadata": {},
   "source": [
    "# RL and Advanced DL: Домашнее задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5250c2b7",
   "metadata": {},
   "source": [
    "Первое ДЗ связано с обучением с подкреплением, и оно придумано для ситуации, когда\n",
    "нейронные сети ещё не нужны, и пространство состояний в целом достаточно маленькое,\n",
    "чтобы можно было обучить хорошую стратегию методами TD-обучения или другими\n",
    "методами обучения с подкреплением. Задание получилось, надеюсь, интересное, но в том\n",
    "числе и достаточно техническое, так что для решения придётся немножко\n",
    "попрограммировать. Поэтому в качестве решения ожидается ссылка на jupyter-ноутбук\n",
    "на вашем github (или публичный, или с доступом для snikolenko); ссылку\n",
    "обязательно нужно прислать в виде сданного домашнего задания на портале\n",
    "Академии. Любые комментарии, новые идеи и рассуждения на тему, как всегда,\n",
    "категорически приветствуются."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8d1e51",
   "metadata": {},
   "source": [
    "## Часть первая, с блекджеком и стратегиями"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2713254",
   "metadata": {},
   "source": [
    "Мы будем обучаться играть в очень простую, но знаменитую и популярную игру: блекджек. Правила блекджека достаточно просты; давайте начнём с самой базовой версии, которая\n",
    "реализована в OpenAI Gym:\n",
    "\n",
    "* численные значения карт равны от 2 до 10 для карт от двойки до десятки, 10 для\n",
    "валетов, дам и королей;\n",
    "* туз считается за 11 очков, если общая сумма карт на руке при этом не превосходит\n",
    "21 (по-английски в этом случае говорят, что на руке есть usable ace), и за 1 очко,\n",
    "если превосходит;\n",
    "* игроку раздаются две карты, дилеру — одна в открытую и одна в закрытую;\n",
    "* игрок может совершать одно из двух действий:\n",
    "    * hit — взять ещё одну карту;\n",
    "    * stand — не брать больше карт;\n",
    "* если сумма очков у игрока на руках больше 21, он проигрывает (bust);\n",
    "* если игрок выбирает stand с суммой не больше 21, дилер добирает карты, пока\n",
    "сумма карт в его руке меньше 17;\n",
    "*  после этого игрок выигрывает, если дилер либо превышает 21, либо получает\n",
    "сумму очков меньше, чем сумма очков у игрока; при равенстве очков объявляется\n",
    "ничья (ставка возвращается);\n",
    "*  в исходных правилах есть ещё дополнительный бонус за natural blackjack: если\n",
    "игрок набирает 21 очко с раздачи, двумя картами, он выигрывает не +1, а +1.5\n",
    "(полторы ставки).\n",
    "\n",
    "\n",
    "Именно этот простейший вариант блекджека реализован в OpenAI Gym:\n",
    "https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "982bba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from blackjack_env_0 import BlackjackEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "098e0d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env = BlackjackEnv(natural=True)\n",
    "    env.reset()\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c5dd326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, pi, eps=0.0, max_steps=1000000):\n",
    "    env.reset()\n",
    "    for step in range(max_steps):\n",
    "        prev_state_id = env.get_obs_id(env._get_obs())\n",
    "        action = pi[prev_state_id] if random.random() > eps else env.action_space.sample()\n",
    "        _, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            return None, reward\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15afa9d3",
   "metadata": {},
   "source": [
    "### 1.1 Рассмотрим очень простую стратегию: говорить stand, если у нас на руках комбинация в 19, 20 или 21 очко, во всех остальных случаях говорить hit. Используйте методы Монте-Карло, чтобы оценить выигрыш от этой стратегии.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b49aca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pi_simple(env):\n",
    "    nS = env.get_states_count()\n",
    "    pi = np.zeros(nS, dtype=int)\n",
    "    for obs in env.get_all_states():\n",
    "        player_card_sum, dealer_card, usable_ace = obs\n",
    "        if player_card_sum in (19, 20, 21):\n",
    "            action = 0 # stand\n",
    "        else:\n",
    "            action = 1 # hit\n",
    "        pi[env.get_obs_id(obs)] = action\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06703f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_reward(env, pi, episode_runner, episodes=100000, verbose=True, *args, **kwargs):\n",
    "    total_reward = 0\n",
    "    for _ in tqdm.tqdm(range(episodes)):\n",
    "        _, reward = episode_runner(env, pi, *args, **kwargs)\n",
    "        total_reward += reward\n",
    "    return total_reward / episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31867f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 100000/100000 [00:16<00:00, 5887.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.18105"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test():\n",
    "    env = make_env()\n",
    "    pi = get_pi_simple(env)\n",
    "    return monte_carlo_reward(env, pi, run_episode)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abd2f0a",
   "metadata": {},
   "source": [
    "### 1.2 Реализуйте метод обучения с подкреплением без модели (можно Q-обучение, но рекомендую попробовать и другие, например Monte Carlo control) для обучения стратегии в блекджеке, используя окружение Blackjack-v0 из OpenAI Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec0942ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_policy_by_Q(Q):\n",
    "    return np.argmax(Q, axis=1)\n",
    "\n",
    "\n",
    "def run_Q_learning_episode(env, pi, Q, alpha=0.05, epsilon=0.05, gamma=0.9, max_steps=1000000):\n",
    "    env.reset()\n",
    "    s = env.get_obs_id(env._get_obs())\n",
    "    a = pi[s] if random.random() > epsilon else env.action_space.sample()\n",
    "    for _ in range(1000):    \n",
    "        obs, reward, done, info = env.step(a)\n",
    "        s_prime = env.get_obs_id(obs)\n",
    "        a_prime = pi[s_prime] if random.random() > epsilon else env.action_space.sample()\n",
    "        Q[s][a] += alpha * (reward + gamma * np.max( Q[s_prime] ) - Q[s][a])\n",
    "        s, a = s_prime, a_prime\n",
    "        if done:\n",
    "            return Q, reward\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def q_learning(env, alpha=0.1, epsilon=0.1, gamma=0.9, episodes=100000, dump_reward_hist=False):\n",
    "    nS = env.get_states_count()\n",
    "    nA = env.get_actions_count()\n",
    "    Q = np.zeros((nS, nA), dtype=float)\n",
    "    pi = compute_policy_by_Q(Q)\n",
    "    \n",
    "    reward_hist = [] if dump_reward_hist else None\n",
    "    msg = f'Train (a={alpha:.4f}, e={epsilon:.4f}, g={gamma:.2f})'\n",
    "    for i in tqdm.tqdm(range(episodes), desc=msg):\n",
    "        Q, _ = run_Q_learning_episode(env, pi, Q, alpha, epsilon, gamma)\n",
    "        pi = compute_policy_by_Q(Q)\n",
    "        if dump_reward_hist:\n",
    "            reward = monte_carlo_reward(env, pi, run_Q_learning_episode, episodes=100000, verbose=False,\n",
    "                                        Q=Q, alpha=alpha, epsilon=epsilon, gamma=gamma)\n",
    "            reward_hist.append(reward)\n",
    "    \n",
    "    return pi, Q, reward_hist\n",
    "\n",
    "\n",
    "def test():\n",
    "    env=make_env()\n",
    "    alpha = 0.005\n",
    "    epsilon = 0.05\n",
    "    gamma = 0.1\n",
    "    pi, Q, _ = q_learning(env, alpha=alpha, epsilon=epsilon, gamma=gamma, episodes=100000)\n",
    "    return monte_carlo_reward(env, pi, run_Q_learning_episode, episodes=100000, verbose=True,\n",
    "                              Q=Q, alpha=alpha, epsilon=epsilon, gamma=gamma)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9241b55",
   "metadata": {},
   "source": [
    "### 1.3 Сколько выигрывает казино у вашей стратегии? Нарисуйте графики среднего дохода вашего метода (усреднённого по крайней мере по 100000 раздач, а лучше больше) по ходу обучения. Попробуйте подобрать оптимальные гиперпараметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57de87ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train (a=0.0020, e=0.0500, g=0.1): 100%|█| 100000/100000 [00:16<00:00, 6085.86it\n",
      "100%|█████████████████████████████████| 100000/100000 [00:19<00:00, 5127.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.063295\n",
      "Best reward: -0.0633.\n",
      "alpha = 0.0020, epsilon = 0.0500, gamma = 0.1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train (a=0.0020, e=0.0500, g=0.1): 100%|█| 100000/100000 [00:16<00:00, 6043.08it\n",
      "100%|█████████████████████████████████| 100000/100000 [00:19<00:00, 5057.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.064205\n",
      "Best reward: -0.0633.\n",
      "alpha = 0.0020, epsilon = 0.0500, gamma = 0.1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train (a=0.0020, e=0.1000, g=0.1): 100%|█| 100000/100000 [00:22<00:00, 4534.10it\n",
      "100%|█████████████████████████████████| 100000/100000 [00:20<00:00, 4988.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.080945\n",
      "Best reward: -0.0633.\n",
      "alpha = 0.0020, epsilon = 0.0500, gamma = 0.1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train (a=0.0020, e=0.1000, g=0.1): 100%|█| 100000/100000 [00:19<00:00, 5199.72it\n",
      "100%|█████████████████████████████████| 100000/100000 [00:22<00:00, 4432.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.079065\n",
      "Best reward: -0.0633.\n",
      "alpha = 0.0020, epsilon = 0.0500, gamma = 0.1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train (a=0.0050, e=0.0500, g=0.1): 100%|█| 100000/100000 [00:18<00:00, 5263.68it\n",
      "100%|█████████████████████████████████| 100000/100000 [00:22<00:00, 4396.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.057915\n",
      "Best reward: -0.0579.\n",
      "alpha = 0.0050, epsilon = 0.0500, gamma = 0.1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train (a=0.0050, e=0.0500, g=0.1): 100%|█| 100000/100000 [00:16<00:00, 6037.86it\n",
      "100%|█████████████████████████████████| 100000/100000 [00:22<00:00, 4447.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.05817\n",
      "Best reward: -0.0579.\n",
      "alpha = 0.0050, epsilon = 0.0500, gamma = 0.1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train (a=0.0050, e=0.1000, g=0.1): 100%|█| 100000/100000 [00:19<00:00, 5260.11it\n",
      "100%|█████████████████████████████████| 100000/100000 [00:19<00:00, 5056.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.06867\n",
      "Best reward: -0.0579.\n",
      "alpha = 0.0050, epsilon = 0.0500, gamma = 0.1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train (a=0.0050, e=0.1000, g=0.1): 100%|█| 100000/100000 [00:19<00:00, 5140.13it\n",
      "100%|█████████████████████████████████| 100000/100000 [00:25<00:00, 3987.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.06538\n",
      "Best reward: -0.0579.\n",
      "alpha = 0.0050, epsilon = 0.0500, gamma = 0.1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train (a=0.0080, e=0.0500, g=0.1): 100%|█| 100000/100000 [00:20<00:00, 4801.58it\n",
      " 32%|██████████▊                       | 31646/100000 [00:07<00:17, 3960.91it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32084/3431471175.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m           f', gamma = {best_gamma:.1f}.')\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mbest_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_32084/3431471175.py\u001b[0m in \u001b[0;36mbest_params\u001b[0;34m(env, train_episodes, eval_episodes)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_episodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         )\n\u001b[0;32m---> 14\u001b[0;31m         avg_reward = monte_carlo_reward(\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_Q_learning_episode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_32084/1141494054.py\u001b[0m in \u001b[0;36mmonte_carlo_reward\u001b[0;34m(env, pi, episode_runner, episodes, verbose, *args, **kwargs)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepisode_runner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_reward\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_32084/4182338685.py\u001b[0m in \u001b[0;36mrun_Q_learning_episode\u001b[0;34m(env, pi, Q, alpha, epsilon, gamma, max_steps)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_Q_learning_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_obs_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/hhd-drive/Yandex.Disk/Computer Science/courses/ml/made-rl-adv-2021/hw/1_blackjack/blackjack_env_0.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdealer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw_hand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw_hand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m33\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_states_mapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/hhd-drive/Yandex.Disk/Computer Science/courses/ml/made-rl-adv-2021/hw/1_blackjack/blackjack_env_0.py\u001b[0m in \u001b[0;36mdraw_hand\u001b[0;34m(np_random)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdraw_hand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdraw_card\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw_card\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/hhd-drive/Yandex.Disk/Computer Science/courses/ml/made-rl-adv-2021/hw/1_blackjack/blackjack_env_0.py\u001b[0m in \u001b[0;36mdraw_card\u001b[0;34m(np_random)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdraw_card\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeck\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def best_params(env, train_episodes, eval_episodes):\n",
    "    alpha_value = np.arange(0.002, 0.010, 0.001)\n",
    "    epsilon_values = [0.05] # np.arange(0.05, 0.15, 0.05)\n",
    "    gamma_values = [0.05] # np.arange(0.05, 0.15, 0.05)\n",
    "    best_reward, best_pi, best_alpha, best_epsilon, best_gamma = -1000, None, None, None, None\n",
    "    for i, (alpha, epsilon, gamma) in enumerate(product(alpha_value, epsilon_values, gamma_values)):\n",
    "        pi, Q, _ = q_learning(\n",
    "            env=env,\n",
    "            alpha=alpha,\n",
    "            epsilon=epsilon,\n",
    "            gamma=gamma,\n",
    "            episodes=train_episodes,\n",
    "        )\n",
    "        avg_reward = monte_carlo_reward(\n",
    "            env, pi, run_Q_learning_episode,\n",
    "            episodes=eval_episodes, verbose=False,\n",
    "            Q=Q, alpha=alpha, epsilon=epsilon, gamma=gamma)\n",
    "        print(avg_reward)\n",
    "        if avg_reward > best_reward:\n",
    "            best_reward = avg_reward\n",
    "            best_pi = pi\n",
    "            best_alpha = alpha\n",
    "            best_epsilon = epsilon\n",
    "            best_gamma = gamma\n",
    "            \n",
    "        print(f'Best reward: {best_reward:.4f}.')\n",
    "        print(f'alpha = {best_alpha:.4f}' \\\n",
    "          f', epsilon = {best_epsilon:.4f}' \\\n",
    "          f', gamma = {best_gamma:.2f}.')\n",
    "    \n",
    "best_params(env=make_env(), train_episodes=100000, eval_episodes=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c15ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "\n",
    "FIGSIZE = (16, 8)\n",
    "FONTSIZE = 16\n",
    "\n",
    "\n",
    "def plot(env, alpha, epsilon, gamma, train_episodes=100000, plot_every=10000):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=FIGSIZE)\n",
    "    \n",
    "    pi, rewards = q_learning(\n",
    "        env=env,\n",
    "        alpha=alpha,\n",
    "        epsilon=epsilon,\n",
    "        gamma=gamma,\n",
    "        episodes=train_episodes,\n",
    "    )\n",
    "    avg_reward = monte_carlo_reward(env, pi, episodes=eval_episodes)\n",
    "    \n",
    "    # Visualization:\n",
    "    ys = [\n",
    "        sum(rewards[i:i + plot_every]) / plot_every\n",
    "        for i in range(0, len(rewards), plot_every)\n",
    "    ]\n",
    "    xs = np.arange(1, len(ys) + 1)\n",
    "    ax.plot(\n",
    "        xs, ys,\n",
    "        label=f'alpha = {alpha:.2f}, epsilon = {epsilon:.2f}, gamma = {gamma:.1f}.',\n",
    "        color=f'C{i}'\n",
    "    )\n",
    "\n",
    "    plt.title('Average reward during training')\n",
    "    plt.xlabel('Episode number')\n",
    "    plt.xticks(xs, xs * plot_every)\n",
    "    plt.ylabel('Current reward')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid()\n",
    "\n",
    "plot(make_env(), alpha=0.01, epsilon=0.2, gamma=0.9, train_episodes=100000, plot_every=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef05da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a2a1e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
