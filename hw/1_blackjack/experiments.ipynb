{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e832a7c",
   "metadata": {},
   "source": [
    "# RL and Advanced DL: Домашнее задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85980d8d",
   "metadata": {},
   "source": [
    "Первое ДЗ связано с обучением с подкреплением, и оно придумано для ситуации, когда\n",
    "нейронные сети ещё не нужны, и пространство состояний в целом достаточно маленькое,\n",
    "чтобы можно было обучить хорошую стратегию методами TD-обучения или другими\n",
    "методами обучения с подкреплением. Задание получилось, надеюсь, интересное, но в том\n",
    "числе и достаточно техническое, так что для решения придётся немножко\n",
    "попрограммировать. Поэтому в качестве решения ожидается ссылка на jupyter-ноутбук\n",
    "на вашем github (или публичный, или с доступом для snikolenko); ссылку\n",
    "обязательно нужно прислать в виде сданного домашнего задания на портале\n",
    "Академии. Любые комментарии, новые идеи и рассуждения на тему, как всегда,\n",
    "категорически приветствуются."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bbd495",
   "metadata": {},
   "source": [
    "## Часть первая, с блекджеком и стратегиями"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee55d19",
   "metadata": {},
   "source": [
    "Мы будем обучаться играть в очень простую, но знаменитую и популярную игру: блекджек. Правила блекджека достаточно просты; давайте начнём с самой базовой версии, которая\n",
    "реализована в OpenAI Gym:\n",
    "\n",
    "* численные значения карт равны от 2 до 10 для карт от двойки до десятки, 10 для\n",
    "валетов, дам и королей;\n",
    "* туз считается за 11 очков, если общая сумма карт на руке при этом не превосходит\n",
    "21 (по-английски в этом случае говорят, что на руке есть usable ace), и за 1 очко,\n",
    "если превосходит;\n",
    "* игроку раздаются две карты, дилеру — одна в открытую и одна в закрытую;\n",
    "* игрок может совершать одно из двух действий:\n",
    "    * hit — взять ещё одну карту;\n",
    "    * stand — не брать больше карт;\n",
    "* если сумма очков у игрока на руках больше 21, он проигрывает (bust);\n",
    "* если игрок выбирает stand с суммой не больше 21, дилер добирает карты, пока\n",
    "сумма карт в его руке меньше 17;\n",
    "*  после этого игрок выигрывает, если дилер либо превышает 21, либо получает\n",
    "сумму очков меньше, чем сумма очков у игрока; при равенстве очков объявляется\n",
    "ничья (ставка возвращается);\n",
    "*  в исходных правилах есть ещё дополнительный бонус за natural blackjack: если\n",
    "игрок набирает 21 очко с раздачи, двумя картами, он выигрывает не +1, а +1.5\n",
    "(полторы ставки).\n",
    "\n",
    "\n",
    "Именно этот простейший вариант блекджека реализован в OpenAI Gym:\n",
    "https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b466fdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from blackjack_env_0 import BlackjackEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1445c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env = BlackjackEnv(natural=True)\n",
    "    env.reset()\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12ecbffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, pi, eps=0.0, max_steps=1000000):\n",
    "    env.reset()\n",
    "    for step in range(max_steps):\n",
    "        prev_state_id = env.get_obs_id(env._get_obs())\n",
    "        action = pi[prev_state_id] if random.random() > eps else env.action_space.sample()\n",
    "        _, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            return None, reward\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1316709",
   "metadata": {},
   "source": [
    "### 1.1 Рассмотрим очень простую стратегию: говорить stand, если у нас на руках комбинация в 19, 20 или 21 очко, во всех остальных случаях говорить hit. Используйте методы Монте-Карло, чтобы оценить выигрыш от этой стратегии.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27722691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pi_simple(env):\n",
    "    nS = env.get_states_count()\n",
    "    pi = np.zeros(nS, dtype=int)\n",
    "    for obs in env.get_all_states():\n",
    "        player_card_sum, dealer_card, usable_ace = obs\n",
    "        if player_card_sum in (19, 20, 21):\n",
    "            action = 0 # stand\n",
    "        else:\n",
    "            action = 1 # hit\n",
    "        pi[env.get_obs_id(obs)] = action\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5f5733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_reward(env, pi, episode_runner, episodes=100000, verbose=True, *args, **kwargs):\n",
    "    total_reward = 0\n",
    "    for _ in tqdm.tqdm(range(episodes)):\n",
    "        _, reward = episode_runner(env, pi, *args, **kwargs)\n",
    "        total_reward += reward\n",
    "    return total_reward / episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a8e39dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 100000/100000 [00:13<00:00, 7293.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.18099"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test():\n",
    "    env = make_env()\n",
    "    pi = get_pi_simple(env)\n",
    "    return monte_carlo_reward(env, pi, run_episode)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619c09df",
   "metadata": {},
   "source": [
    "### 1.2 Реализуйте метод обучения с подкреплением без модели (можно Q-обучение, но рекомендую попробовать и другие, например Monte Carlo control) для обучения стратегии в блекджеке, используя окружение Blackjack-v0 из OpenAI Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec0e4bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train (a=0.00, e=0.97, g=1.00):   0%|      | 5/100000 [00:00<00:35, 2782.48it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(24, 7, False)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31903/2257415057.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     return monte_carlo_reward(env, pi, run_Q_learning_episode, episodes=10000, verbose=True,\n\u001b[1;32m     46\u001b[0m                               Q=Q, alpha=alpha, epsilon=epsilon, gamma=gamma)\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_31903/2257415057.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9684817205319124\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     return monte_carlo_reward(env, pi, run_Q_learning_episode, episodes=10000, verbose=True,\n\u001b[1;32m     46\u001b[0m                               Q=Q, alpha=alpha, epsilon=epsilon, gamma=gamma)\n",
      "\u001b[0;32m/tmp/ipykernel_31903/2257415057.py\u001b[0m in \u001b[0;36mq_learning\u001b[0;34m(env, alpha, epsilon, gamma, episodes, dump_reward_hist)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'Train (a={alpha:.2f}, e={epsilon:.2f}, g={gamma:.2f})'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_Q_learning_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_policy_by_Q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdump_reward_hist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31903/2257415057.py\u001b[0m in \u001b[0;36mrun_Q_learning_episode\u001b[0;34m(env, pi, Q, alpha, epsilon, gamma, max_steps)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0ms_prime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_obs_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0ma_prime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms_prime\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms_prime\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/hhd-drive/Yandex.Disk/Computer Science/courses/ml/made-rl-adv-2021/hw/1_blackjack/blackjack_env_0.py\u001b[0m in \u001b[0;36mget_obs_id\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_obs_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_states_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_all_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (24, 7, False)"
     ]
    }
   ],
   "source": [
    "def compute_policy_by_Q(Q):\n",
    "    return np.argmax(Q, axis=1)\n",
    "\n",
    "\n",
    "def run_Q_learning_episode(env, pi, Q, alpha=0.05, epsilon=0.05, gamma=0.9, max_steps=1000000):\n",
    "    env.reset()\n",
    "    s = env.get_obs_id(env._get_obs())\n",
    "    a = pi[s] if random.random() > epsilon else env.action_space.sample()\n",
    "    for _ in range(1000):    \n",
    "        obs, reward, done, info = env.step(a)\n",
    "        s_prime = env.get_obs_id(obs)\n",
    "        a_prime = pi[s_prime] if random.random() > epsilon else env.action_space.sample()\n",
    "        Q[s][a] = Q[s][a] + alpha * (reward + gamma * np.max( Q[s_prime] ) - Q[s][a])\n",
    "        s, a = s_prime, a_prime\n",
    "        if done:\n",
    "            return Q, reward\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def q_learning(env, alpha=0.1, epsilon=0.1, gamma=0.9, episodes=100000, dump_reward_hist=False):\n",
    "    nS = env.get_states_count()\n",
    "    nA = env.get_actions_count()\n",
    "    Q = np.zeros((nS, nA), dtype=float)\n",
    "    pi = compute_policy_by_Q(Q)\n",
    "    \n",
    "    reward_hist = [] if dump_reward_hist else None\n",
    "    msg = f'Train (a={alpha:.2f}, e={epsilon:.2f}, g={gamma:.2f})'\n",
    "    for i in tqdm.tqdm(range(episodes), desc=msg):\n",
    "        Q, _ = run_Q_learning_episode(env, pi, Q, alpha, epsilon, gamma)\n",
    "        pi = compute_policy_by_Q(Q)\n",
    "        if dump_reward_hist:\n",
    "            reward = monte_carlo_reward(env, pi, run_Q_learning_episode, episodes=100000, verbose=False,\n",
    "                                        Q=Q, alpha=alpha, epsilon=epsilon, gamma=gamma)\n",
    "            reward_hist.append(reward)\n",
    "    \n",
    "    return pi, Q, reward_hist\n",
    "\n",
    "\n",
    "def test():\n",
    "    env=make_env()\n",
    "    alpha = 0.0014975518853093716\n",
    "    epsilon = 0.9684817205319124\n",
    "    gamma = 1\n",
    "    pi, Q, _ = q_learning(env, alpha=alpha, epsilon=epsilon, gamma=gamma, episodes=100000)\n",
    "    return monte_carlo_reward(env, pi, run_Q_learning_episode, episodes=10000, verbose=True,\n",
    "                              Q=Q, alpha=alpha, epsilon=epsilon, gamma=gamma)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d8434e",
   "metadata": {},
   "source": [
    "### 1.3 Сколько выигрывает казино у вашей стратегии? Нарисуйте графики среднего дохода вашего метода (усреднённого по крайней мере по 100000 раздач, а лучше больше) по ходу обучения. Попробуйте подобрать оптимальные гиперпараметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b81a9a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train (a=0.10, e=0.05, g=0.90): 100%|█| 100000/100000 [00:12<00:00, 8234.19it/s]\n",
      "100%|███████████████████████████████████| 10000/10000 [00:01<00:00, 8061.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.166\n",
      "Best reward: -0.1660.\n",
      "alpha = 0.10, epsilon = 0.05, gamma = 0.9.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train (a=0.10, e=0.15, g=0.90): 100%|█| 100000/100000 [00:11<00:00, 8427.42it/s]\n",
      "100%|███████████████████████████████████| 10000/10000 [00:01<00:00, 8046.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.19965\n",
      "Best reward: -0.1660.\n",
      "alpha = 0.10, epsilon = 0.05, gamma = 0.9.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train (a=0.10, e=0.25, g=0.90): 100%|█| 100000/100000 [00:12<00:00, 8252.29it/s]\n",
      "100%|███████████████████████████████████| 10000/10000 [00:01<00:00, 7360.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.19755\n",
      "Best reward: -0.1660.\n",
      "alpha = 0.10, epsilon = 0.05, gamma = 0.9.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train (a=0.10, e=0.35, g=0.90): 100%|█| 100000/100000 [00:12<00:00, 8110.41it/s]\n",
      "100%|███████████████████████████████████| 10000/10000 [00:01<00:00, 7896.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.21315\n",
      "Best reward: -0.1660.\n",
      "alpha = 0.10, epsilon = 0.05, gamma = 0.9.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train (a=0.10, e=0.45, g=0.90): 100%|█| 100000/100000 [00:12<00:00, 7969.30it/s]\n",
      "100%|███████████████████████████████████| 10000/10000 [00:01<00:00, 8090.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.249\n",
      "Best reward: -0.1660.\n",
      "alpha = 0.10, epsilon = 0.05, gamma = 0.9.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train (a=0.10, e=0.55, g=0.90): 100%|█| 100000/100000 [00:12<00:00, 8007.52it/s]\n",
      "100%|███████████████████████████████████| 10000/10000 [00:01<00:00, 6831.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.24755\n",
      "Best reward: -0.1660.\n",
      "alpha = 0.10, epsilon = 0.05, gamma = 0.9.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train (a=0.15, e=0.05, g=0.90): 100%|█| 100000/100000 [00:11<00:00, 8456.58it/s]\n",
      "100%|███████████████████████████████████| 10000/10000 [00:03<00:00, 2528.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.1745\n",
      "Best reward: -0.1660.\n",
      "alpha = 0.10, epsilon = 0.05, gamma = 0.9.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train (a=0.15, e=0.15, g=0.90): 100%|█| 100000/100000 [00:12<00:00, 8274.71it/s]\n",
      "100%|███████████████████████████████████| 10000/10000 [00:01<00:00, 8410.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.1678\n",
      "Best reward: -0.1660.\n",
      "alpha = 0.10, epsilon = 0.05, gamma = 0.9.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train (a=0.15, e=0.25, g=0.90): 100%|█| 100000/100000 [00:14<00:00, 6737.77it/s]\n",
      "100%|███████████████████████████████████| 10000/10000 [00:04<00:00, 2499.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.2196\n",
      "Best reward: -0.1660.\n",
      "alpha = 0.10, epsilon = 0.05, gamma = 0.9.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train (a=0.15, e=0.35, g=0.90): 100%|█| 100000/100000 [00:15<00:00, 6353.05it/s]\n",
      "100%|███████████████████████████████████| 10000/10000 [00:03<00:00, 2675.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.2054\n",
      "Best reward: -0.1660.\n",
      "alpha = 0.10, epsilon = 0.05, gamma = 0.9.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train (a=0.15, e=0.45, g=0.90): 100%|█| 100000/100000 [00:12<00:00, 7868.42it/s]\n",
      "100%|███████████████████████████████████| 10000/10000 [00:01<00:00, 7900.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.22855\n",
      "Best reward: -0.1660.\n",
      "alpha = 0.10, epsilon = 0.05, gamma = 0.9.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train (a=0.15, e=0.55, g=0.90): 100%|█| 100000/100000 [00:12<00:00, 7816.09it/s]\n",
      "100%|███████████████████████████████████| 10000/10000 [00:01<00:00, 7292.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.25775\n",
      "Best reward: -0.1660.\n",
      "alpha = 0.10, epsilon = 0.05, gamma = 0.9.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def best_params(env, train_episodes, eval_episodes):\n",
    "    alpha_value = np.arange(0.1, 0.2, 0.05)\n",
    "    epsilon_values = np.arange(0.05, 0.65, 0.1)\n",
    "    gamma_values = [0.9] # np.arange(0.1, 1.0, 0.4)\n",
    "    best_reward, best_pi, best_alpha, best_epsilon, best_gamma = -1000, None, None, None, None\n",
    "    for i, (alpha, epsilon, gamma) in enumerate(product(alpha_value, epsilon_values, gamma_values)):\n",
    "        pi, Q, _ = q_learning(\n",
    "            env=env,\n",
    "            alpha=alpha,\n",
    "            epsilon=epsilon,\n",
    "            gamma=gamma,\n",
    "            episodes=train_episodes,\n",
    "        )\n",
    "        avg_reward = monte_carlo_reward(\n",
    "            env, pi, run_Q_learning_episode,\n",
    "            episodes=eval_episodes, verbose=False,\n",
    "            Q=Q, alpha=alpha, epsilon=epsilon, gamma=gamma)\n",
    "        print(avg_reward)\n",
    "        if avg_reward > best_reward:\n",
    "            best_reward = avg_reward\n",
    "            best_pi = pi\n",
    "            best_alpha = alpha\n",
    "            best_epsilon = epsilon\n",
    "            best_gamma = gamma\n",
    "            \n",
    "        print(f'Best reward: {best_reward:.4f}.')\n",
    "        print(f'alpha = {best_alpha:.2f}' \\\n",
    "          f', epsilon = {best_epsilon:.2f}' \\\n",
    "          f', gamma = {best_gamma:.1f}.')\n",
    "    \n",
    "best_params(env=make_env(), train_episodes=100000, eval_episodes=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ef1981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "\n",
    "FIGSIZE = (16, 8)\n",
    "FONTSIZE = 16\n",
    "\n",
    "\n",
    "def plot(env, alpha, epsilon, gamma, train_episodes=100000, plot_every=10000):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=FIGSIZE)\n",
    "    \n",
    "    pi, rewards = q_learning(\n",
    "        env=env,\n",
    "        alpha=alpha,\n",
    "        epsilon=epsilon,\n",
    "        gamma=gamma,\n",
    "        episodes=train_episodes,\n",
    "    )\n",
    "    avg_reward = monte_carlo_reward(env, pi, episodes=eval_episodes)\n",
    "    \n",
    "    # Visualization:\n",
    "    ys = [\n",
    "        sum(rewards[i:i + plot_every]) / plot_every\n",
    "        for i in range(0, len(rewards), plot_every)\n",
    "    ]\n",
    "    xs = np.arange(1, len(ys) + 1)\n",
    "    ax.plot(\n",
    "        xs, ys,\n",
    "        label=f'alpha = {alpha:.2f}, epsilon = {epsilon:.2f}, gamma = {gamma:.1f}.',\n",
    "        color=f'C{i}'\n",
    "    )\n",
    "\n",
    "    plt.title('Average reward during training')\n",
    "    plt.xlabel('Episode number')\n",
    "    plt.xticks(xs, xs * plot_every)\n",
    "    plt.ylabel('Current reward')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid()\n",
    "\n",
    "plot(make_env(), alpha=0.01, epsilon=0.2, gamma=0.9, train_episodes=100000, plot_every=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45131ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e2fbf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
