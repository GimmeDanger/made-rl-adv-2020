# RL and Advanced DL at MADE, 2021

Это курс об обучении с подкреплением и порождающих моделях в глубоком обучении, представленный в [Академии больших данных Mail.Ru](https://data.mail.ru/) в 2021 году. <!--- [Все материалы](https://logic.pdmi.ras.ru/~sergey/teaching/maderl2021.html)-->

# Лекции
1. Введение в обучение с подкреплением. Многорукие бандиты  <!--- :[запись](https://www.youtube.com/watch?v=xW1dxZNeWWk)-->
2. Конкурентные и контекстуальные бандиты. Обучение с подкреплением: определение MDP, уравнения Беллмана. Сложности в решении уравнений Беллмана. Policy iteration  <!--- :[запись](https://www.youtube.com/watch?v=sVehB6JgWIs)-->
3. Обучение стратегий без модели: методы Монте-Карло, от MC estimation к MC control. On-policy и off-policy методы. TD-обучение: Sarsa (on-policy TD-обучение) и Q-learning (off-policy TD-обучение) <!--- :[запись](https://youtu.be/4chjddQ3UVk)-->
4. Обобщения TD и MC-методов: n-step RL и так далее. Планирование: rollouts, MCTS <!--- :[запись](https://youtu.be/ZHImFAYg0xk)-->
5. Приближённые методы в RL и градиент по стратегиям (policy gradient) <!--- :[запись](https://youtu.be/s1qzIlCNeVU)-->
6. Глубокое Q-обучение: DQN и прочие трюки. Компьютерное го: история, AlphaGo, AlphaZero, MuZero <!--- :[запись](https://www.youtube.com/watch?v=530reZ6pSVg)-->
7. Современное обучение с подкреплением: параллельные реализации, DDPG, TRPO и PPO, REINFORCE вне RL, автоматический поиск архитектур, Dactyl и синтетические данные, мультиагентные задачи и autocurriculum, world models, successor features и композициональность в RL <!--- :[запись](https://www.youtube.com/watch?v=qqIWiqwZZXc)-->
8. Порождающие и дискриминирующие модели в машинном обучении: наивный Байес и логистическая регрессия, HMM и CRF. Порождающие модели в DL. Авторегрессионные модели с явным представлением плотности: MADE, PixelRNN, PixelCNN <!--- :[запись](https://www.youtube.com/watch?v=v2-E8QofIkw)-->
9. Порождающие состязательные сети (GAN): основные идеи, примеры, DCGAN, ProGAN. Основные состязательные функции ошибки: LSGAN и Wassershtein GAN  <!--- :[запись](https://www.youtube.com/watch?v=zIwTZMm8mSQ)-->
10. Порождающие состязательные сети (GAN) II: EBGAN, условные GAN, AAE и другие идеи. Case study: перенос стиля GAN'ами и не только <!--- :[запись](https://www.youtube.com/watch?v=D8882XpGVoc)-->
11. Вариационные автокодировщики от основ до VQ-VAE <!--- :[запись](https://www.youtube.com/watch?v=3TyFcbhDFPw)-->
12. Transformer и что из него получилось: BERT, GPT и DALL-E <!--- :[запись](https://www.youtube.com/watch?v=FiDa5OFccz8)-->
13. Порождающие модели, основанные на потоках <!--- :[запись](https://www.youtube.com/watch?v=N6AQhMn0-ec)-->

# Домашние задания
1. Учимся обыгрывать казино: RL с блекджеком и Дастином Хоффманом.
2. Играем в крестики-нолики: Q-обучение, DQN, MCTS и (может быть) AlphaZero.
3. Перенос стиля при помощи pix2pix и CycleGAN.

# Selected references
1. Richard S. Sutton, Andrew G. Barto.  Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 2018.
